{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf5d27a",
   "metadata": {},
   "source": [
    "# Data preparation and transformation exercise\n",
    "\n",
    "## Part III - Missing values and Imputation\n",
    "\n",
    "The objective of this exercise is to practice various steps of data preprocessing and feature engineering.\n",
    "\n",
    "The scenario is the preparation of data for a ML multilinear regressions.\n",
    "\n",
    "The dataset used is the \"Climate Weather Surface of Brazil - Hourly\", wich is available at <a href=\"https://www.kaggle.com/PROPPG-PPG/hourly-weather-surface-brazil-southeast-region?select=make_dataset.py\">Kaggle</a>.\n",
    "\n",
    "It contains hourly climate data taken from 122 weather stations in Brasil between 2000 and 2021.\n",
    "\n",
    "**Steps:**\n",
    "1. Load data\n",
    "2. Inspect data\n",
    "3. Format features\n",
    "4. Clean messy data\n",
    "5. Remove duplicate values\n",
    "6. <a href=\"#Treat-missing-values\">Treat missing values</a>\n",
    "7. <a href=\"#Imputation\">Imputation</a>\n",
    "8. Remove strongly correlated features\n",
    "9. Remove outliers\n",
    "10. Aggregate features\n",
    "11. Encode categorical features\n",
    "12. Feature scaling\n",
    "13. Dimensionality reduction and feature decomposition\n",
    "14. Sample and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbb5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "dataset = pickle.load(open(\"clean_dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88004998",
   "metadata": {},
   "source": [
    "## Treat missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb747fa1",
   "metadata": {},
   "source": [
    "As seen above, missing values were replaced with -9999 (solar_radiation) or -9999.0 (other features).\n",
    "\n",
    "If all sensor values are simultaneously missing, we will discard the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0e2414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1468158 empty measurements\n"
     ]
    }
   ],
   "source": [
    "duplicates = dataset.loc[\n",
    "    (dataset.precipitation == -9999.0) & \n",
    "    (dataset.pressure == -9999.0) & \n",
    "    (dataset.pressure_max == -9999.0) & \n",
    "    (dataset.pressure_min == -9999.0) & \n",
    "    (dataset.solar_radiation == -9999) & \n",
    "    (dataset.air_temperature == -9999.0) & \n",
    "    (dataset.dp_temperature == -9999.0) & \n",
    "    (dataset.air_temp_max == -9999.0) & \n",
    "    (dataset.air_temp_min == -9999.0) & \n",
    "    (dataset.wind_gust == -9999.0) & \n",
    "    (dataset.wind_speed == -9999.0)].index\n",
    "print(\"The dataset has {} empty measurements\".format(len(duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7e4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the duplicates\n",
    "dataset.drop(index=duplicates, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e627313",
   "metadata": {},
   "source": [
    "The empty readings are gone. Next we will replace individual missing readings (-9999) with np.nan, to facilitate their tratment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d60f5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nines(col_name :str):\n",
    "    \"\"\"\n",
    "    Replace the values -9999 in the column specified of the (global) dataset with np.nan\n",
    "    \"\"\"\n",
    "    global dataset\n",
    "    \n",
    "    dataset.loc[dataset[col_name] == -9999.0, col_name] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3351ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"precipitation\",\n",
    "                    \"pressure\",\n",
    "                    \"pressure_max\",\n",
    "                    \"pressure_min\",\n",
    "                    \"solar_radiation\",\n",
    "                    \"air_temperature\",\n",
    "                    \"dp_temperature\",\n",
    "                    \"air_temp_max\",\n",
    "                    \"air_temp_min\",\n",
    "                    \"dp_temp_max\",\n",
    "                    \"dp_temp_min\",\n",
    "                    \"rel_hum_max\",\n",
    "                    \"rel_hum_min\",\n",
    "                    \"Rel_humidity\",\n",
    "                    \"wind_direction\",\n",
    "                    \"wind_gust\",\n",
    "                    \"wind_speed\"]\n",
    "\n",
    "for i in numeric_features:\n",
    "    replace_nines(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b232f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open(\"with_nan_dataset.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20207194",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccf939",
   "metadata": {},
   "source": [
    "Next we will use the mean value of the feature in each station to replace the missing values of that feature.\n",
    "\n",
    "**Warning:** we should not calculate the mean of the training dataset using values from the test dataset. However, in order to keep this exercise simple, we will ignore this and impute the missing values before spliting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a29172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_time                0\n",
      "precipitation       569786\n",
      "pressure             81912\n",
      "pressure_max         86194\n",
      "pressure_min         87165\n",
      "solar_radiation    4606596\n",
      "air_temperature      19866\n",
      "dp_temperature      221481\n",
      "air_temp_max         26916\n",
      "air_temp_min         27768\n",
      "dp_temp_max         224436\n",
      "dp_temp_min         231134\n",
      "rel_hum_max         214905\n",
      "rel_hum_min         220026\n",
      "Rel_humidity        209843\n",
      "wind_direction      230632\n",
      "wind_gust           154709\n",
      "wind_speed          142705\n",
      "station_name             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count missing values per feature\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f69f2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing station 1 PARANOA (COOPA-DF)\n",
      "processing station 2 BELA VISTA\n",
      "processing station 3 NOVA UBIRATA\n",
      "processing station 4 GOIANIA\n",
      "processing station 5 CARLINDA\n",
      "processing station 6 SIDROLANDIA\n",
      "processing station 7 ITAQUIRAI\n",
      "processing station 8 COTRIGUACU\n",
      "processing station 9 SAO MIGUEL DO ARAGUAIA\n",
      "processing station 10 NHUMIRIM\n",
      "processing station 11 QUERENCIA\n",
      "processing station 12 RIO VERDE\n",
      "processing station 13 APIACAS\n",
      "processing station 14 SALTO DO CEU\n",
      "processing station 15 COXIM\n",
      "processing station 16 SAO GABRIEL DO OESTE\n",
      "processing station 17 PONTA PORA\n",
      "processing station 18 MARACAJU\n",
      "processing station 19 ALTO PARAISO DE GOIAS\n",
      "processing station 20 BRASNORTE (NOVO MUNDO)\n",
      "processing station 21 JATAI\n",
      "processing station 22 GAUCHA DO NORTE\n",
      "processing station 23 SAO FELIX  DO ARAGUAIA\n",
      "processing station 24 SANTO ANTONIO DO LESTE\n",
      "processing station 25 PARANATINGA\n",
      "processing station 26 SORRISO\n",
      "processing station 27 DOURADOS\n",
      "processing station 28 GUARANTA DO NORTE\n",
      "processing station 29 AGUA CLARA\n",
      "processing station 30 JUTI\n",
      "processing station 31 TRES LAGOAS\n",
      "processing station 32 MONTE ALEGRE DE GOIAS\n",
      "processing station 33 TANGARA DA SERRA\n",
      "processing station 34 CAIAPONIA\n",
      "processing station 35 PRIMAVERA DO LESTE\n",
      "processing station 36 POSSE\n",
      "processing station 37 SETE QUEDAS\n",
      "processing station 38 BATAGUASSU\n",
      "processing station 39 S.J. DO RIO CLARO\n",
      "processing station 40 PORTO ESTRELA\n",
      "processing station 41 RONDONOPOLIS\n",
      "processing station 42 IPORA\n",
      "processing station 43 CACERES\n",
      "processing station 44 AMAMBAI\n",
      "processing station 45 SAPEZAL\n",
      "processing station 46 BRAZLANDIA\n",
      "processing station 47 ITIQUIRA\n",
      "processing station 48 CUIABA\n",
      "processing station 49 ALTO TAQUARI\n",
      "processing station 50 PORTO MURTINHO\n",
      "processing station 51 GOIAS\n",
      "processing station 52 ALTA FLORESTA\n",
      "processing station 53 LUZIANIA\n",
      "processing station 54 CATALAO\n",
      "processing station 55 MINEIROS\n",
      "processing station 56 CHAPADAO DO SUL\n",
      "processing station 57 GOIANESIA\n",
      "processing station 58 RIO BRILHANTE\n",
      "processing station 59 ALTO ARAGUAIA\n",
      "processing station 60 GAMA (PONTE ALTA)\n",
      "processing station 61 SONORA\n",
      "processing station 62 AGUAS EMENDADAS\n",
      "processing station 63 BANDEIRANTES\n",
      "processing station 64 CASSILANDIA\n",
      "processing station 65 NOVA MARINGA\n",
      "processing station 66 SELVIRIA\n",
      "processing station 67 CRISTALINA (FAZENDA SANTA MONICA)\n",
      "processing station 68 JUARA\n",
      "processing station 69 MIRANDA\n",
      "processing station 70 CAMPO NOVO DOS PARECIS\n",
      "processing station 71 ITAPACI\n",
      "processing station 72 MORRINHOS\n",
      "processing station 73 PONTES E LACERDA\n",
      "processing station 74 ARAL MOREIRA\n",
      "processing station 75 BRASILANDIA\n",
      "processing station 76 SILVANIA\n",
      "processing station 77 ITUMBIARA\n",
      "processing station 78 CAMPO GRANDE\n",
      "processing station 79 AQUIDAUANA\n",
      "processing station 80 NOVA ALVORADA DO SUL\n",
      "processing station 81 COSTA RICA\n",
      "processing station 82 CRISTALINA\n",
      "processing station 83 SINOP\n",
      "processing station 84 JARDIM\n",
      "processing station 85 VILA BELA DA SANTISSIMA TRINDADE\n",
      "processing station 86 CAMAPUA\n",
      "processing station 87 CAMPO VERDE\n",
      "processing station 88 BRASILIA\n",
      "processing station 89 AGUA BOA\n",
      "processing station 90 CORUMBA\n",
      "processing station 91 COMODORO\n",
      "processing station 92 PARANAIBA\n",
      "processing station 93 ITAPORA\n",
      "processing station 94 PARAUNA\n",
      "processing station 95 ANGELICA\n",
      "processing station 96 CAARAPO\n",
      "processing station 97 ARAGARCAS\n",
      "processing station 98 SAO SIMAO\n",
      "processing station 99 PIRES DO RIO\n",
      "processing station 100 GUIRATINGA\n",
      "processing station 101 PORANGATU\n",
      "processing station 102 IGUATEMI\n",
      "processing station 103 PEDRO GOMES\n",
      "processing station 104 FATIMA DO SUL\n",
      "processing station 105 RIBAS DO RIO PARDO\n",
      "processing station 106 EDEIA\n",
      "processing station 107 LAGUNA CARAPA\n",
      "processing station 108 NOVA ANDRADINA\n",
      "processing station 109 SANTA RITA DO PARDO\n",
      "processing station 110 JUINA\n",
      "processing station 111 BONITO\n",
      "processing station 112 IVINHEMA\n",
      "processing station 113 ROSARIO OESTE\n",
      "processing station 114 SAO JOSE DO XINGU\n",
      "processing station 115 SERRA NOVA DOURADA\n"
     ]
    }
   ],
   "source": [
    "# we'll generate a dataset (frame) with the data of each station\n",
    "# and use them to calculate the mean to imputate missing values\n",
    "frames = []\n",
    "\n",
    "# number of numeric features present in the dataset\n",
    "num_features = 17\n",
    "\n",
    "# position offset for the first numeric feature within datates\n",
    "feature_offset = 1\n",
    "\n",
    "# list with names of all the stations\n",
    "stations = dataset['station_name'].unique().tolist()\n",
    "\n",
    "a = 1\n",
    "for i in stations: # iterate stations\n",
    "    # create dataset with data of that station only\n",
    "    df_station = dataset[dataset['station_name']== i].copy()\n",
    "    print('processing station {} {}'.format(str(a),i))\n",
    "    a +=1\n",
    "    for j in range(feature_offset, feature_offset + num_features): # iterate features\n",
    "        df_station[dataset.columns[j]].fillna(df_station[dataset.columns[j]].mean(),inplace = True)\n",
    "    frames.append(df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02fd0f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a35727dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the dataset from the cleaned frames\n",
    "dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca2c0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open(\"imputed_dataset.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0e3fe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_time          0\n",
      "precipitation      0\n",
      "pressure           0\n",
      "pressure_max       0\n",
      "pressure_min       0\n",
      "solar_radiation    0\n",
      "air_temperature    0\n",
      "dp_temperature     0\n",
      "air_temp_max       0\n",
      "air_temp_min       0\n",
      "dp_temp_max        0\n",
      "dp_temp_min        0\n",
      "rel_hum_max        0\n",
      "rel_hum_min        0\n",
      "Rel_humidity       0\n",
      "wind_direction     0\n",
      "wind_gust          0\n",
      "wind_speed         0\n",
      "station_name       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# confirm that we don not have missing values\n",
    "print(dataset.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
